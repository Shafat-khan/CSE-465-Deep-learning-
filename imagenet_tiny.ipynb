{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9N5z0qiT5RV",
        "outputId": "71116acc-0de8-44b5-8f3d-a02280726585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/akash2sharma/tiny-imagenet?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 474M/474M [00:24<00:00, 20.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from shutil import copyfile\n",
        "\n",
        "# Install kagglehub and import it\n",
        "!pip install kagglehub\n",
        "import kagglehub\n",
        "\n",
        "# Download the dataset using kagglehub\n",
        "path = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from shutil import copyfile\n",
        "\n",
        "# Check for GPU availability\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "if use_gpu:\n",
        "    print(\"Using CUDA\")\n",
        "else:\n",
        "    print(\"Not Using CUDA\")\n",
        "\n",
        "# Dataset directories\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/akash2sharma/tiny-imagenet/versions/1\"\n",
        "train_dir = os.path.join(dataset_path, 'tiny-imagenet-200', 'tiny-imagenet-200', 'train')\n",
        "val_dir = os.path.join(dataset_path, 'tiny-imagenet-200', 'tiny-imagenet-200', 'val')\n",
        "val_annotations_file = os.path.join(val_dir, \"val_annotations.txt\")\n",
        "val_images_dir = os.path.join(val_dir, \"images\")\n",
        "organized_val_dir = \"/content/tiny-imagenet-val-organized/\"  # Temporary directory\n",
        "\n",
        "# Number of classes and batch size\n",
        "num_classes = 200  # Tiny ImageNet has 200 classes\n",
        "batch_size = 128\n",
        "num_epochs = 12\n",
        "feature_extract = True\n",
        "train_model = True\n",
        "\n",
        "# Transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Organize validation dataset\n",
        "if not os.path.exists(organized_val_dir):\n",
        "    os.makedirs(organized_val_dir)\n",
        "    print(f\"Created directory: {organized_val_dir}\")\n",
        "\n",
        "with open(val_annotations_file, \"r\") as f:\n",
        "    for line in f.readlines():\n",
        "        parts = line.split(\"\\t\")\n",
        "        img_name, class_id = parts[0], parts[1]\n",
        "        class_dir = os.path.join(organized_val_dir, class_id)\n",
        "        if not os.path.exists(class_dir):\n",
        "            os.makedirs(class_dir)\n",
        "        src_path = os.path.join(val_images_dir, img_name)\n",
        "        dest_path = os.path.join(class_dir, img_name)\n",
        "        if os.path.exists(src_path):\n",
        "            copyfile(src_path, dest_path)\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(organized_val_dir, transform=data_transforms['val'])\n",
        "\n",
        "# Debugging: Ensure labels are in the correct range\n",
        "print(f\"Train dataset classes: {train_dataset.classes}\")\n",
        "print(f\"Validation dataset classes: {val_dataset.classes}\")\n",
        "\n",
        "# Weighted Sampler for balancing classes\n",
        "class_counts = np.bincount([s[1] for s in train_dataset.samples])\n",
        "class_weights = 1. / class_counts\n",
        "samples_weights = [class_weights[label] for _, label in train_dataset.samples]\n",
        "sampler = WeightedRandomSampler(samples_weights, len(samples_weights), replacement=True)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
        "\n",
        "# Hybrid Model Definition\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes, use_pretrained=True, feature_extract=True):\n",
        "        super(HybridModel, self).__init__()\n",
        "\n",
        "        # VGG19\n",
        "        self.vgg = models.vgg19_bn(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vgg, feature_extract)\n",
        "        num_ftrs_vgg = self.vgg.classifier[6].in_features\n",
        "        self.vgg.classifier[6] = nn.Linear(num_ftrs_vgg, num_classes)\n",
        "\n",
        "        # Vision Transformer\n",
        "        self.vit = models.vit_b_16(pretrained=use_pretrained)\n",
        "        self.set_parameter_requires_grad(self.vit, feature_extract)\n",
        "        num_ftrs_vit = self.vit.heads.head.in_features\n",
        "        self.vit.heads.head = nn.Linear(num_ftrs_vit, num_classes)\n",
        "\n",
        "        # Combined classifier\n",
        "        self.classifier = nn.Linear(num_classes * 2, num_classes)\n",
        "\n",
        "    def set_parameter_requires_grad(self, model, feature_extracting):\n",
        "        if feature_extracting:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        vgg_out = self.vgg(x)\n",
        "        vit_out = self.vit(x)\n",
        "        combined_out = torch.cat((vgg_out, vit_out), dim=1)\n",
        "        output = self.classifier(combined_out)\n",
        "        return output\n",
        "\n",
        "# Initialize the model\n",
        "model_name = \"hybrid_vgg_vit\"\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    if model_name == \"hybrid_vgg_vit\":\n",
        "        model = HybridModel(num_classes, use_pretrained, feature_extract)\n",
        "        input_size = 224\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model name\")\n",
        "    return model, input_size\n",
        "\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Optimizer, Loss Function, Scheduler\n",
        "params_to_update = model_ft.parameters()\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=4, gamma=0.1)\n",
        "\n",
        "# Training function with error checks\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, patience=5):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Debugging: Check the label range\n",
        "                assert labels.max() < num_classes, f\"Invalid label found: {labels.max()}\"\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Save best model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                early_stop_counter = 0\n",
        "            elif phase == 'val':\n",
        "                early_stop_counter += 1\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    print('Training complete')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model_ft = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, scheduler, num_epochs=num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtPvnybUUZ3O",
        "outputId": "68993c66-ac6d-45d8-aa73-4d192dbe630b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA\n",
            "Created directory: /content/tiny-imagenet-val-organized/\n",
            "Train dataset classes: ['n01443537', 'n01629819', 'n01641577', 'n01644900', 'n01698640', 'n01742172', 'n01768244', 'n01770393', 'n01774384', 'n01774750', 'n01784675', 'n01855672', 'n01882714', 'n01910747', 'n01917289', 'n01944390', 'n01945685', 'n01950731', 'n01983481', 'n01984695', 'n02002724', 'n02056570', 'n02058221', 'n02074367', 'n02085620', 'n02094433', 'n02099601', 'n02099712', 'n02106662', 'n02113799', 'n02123045', 'n02123394', 'n02124075', 'n02125311', 'n02129165', 'n02132136', 'n02165456', 'n02190166', 'n02206856', 'n02226429', 'n02231487', 'n02233338', 'n02236044', 'n02268443', 'n02279972', 'n02281406', 'n02321529', 'n02364673', 'n02395406', 'n02403003', 'n02410509', 'n02415577', 'n02423022', 'n02437312', 'n02480495', 'n02481823', 'n02486410', 'n02504458', 'n02509815', 'n02666196', 'n02669723', 'n02699494', 'n02730930', 'n02769748', 'n02788148', 'n02791270', 'n02793495', 'n02795169', 'n02802426', 'n02808440', 'n02814533', 'n02814860', 'n02815834', 'n02823428', 'n02837789', 'n02841315', 'n02843684', 'n02883205', 'n02892201', 'n02906734', 'n02909870', 'n02917067', 'n02927161', 'n02948072', 'n02950826', 'n02963159', 'n02977058', 'n02988304', 'n02999410', 'n03014705', 'n03026506', 'n03042490', 'n03085013', 'n03089624', 'n03100240', 'n03126707', 'n03160309', 'n03179701', 'n03201208', 'n03250847', 'n03255030', 'n03355925', 'n03388043', 'n03393912', 'n03400231', 'n03404251', 'n03424325', 'n03444034', 'n03447447', 'n03544143', 'n03584254', 'n03599486', 'n03617480', 'n03637318', 'n03649909', 'n03662601', 'n03670208', 'n03706229', 'n03733131', 'n03763968', 'n03770439', 'n03796401', 'n03804744', 'n03814639', 'n03837869', 'n03838899', 'n03854065', 'n03891332', 'n03902125', 'n03930313', 'n03937543', 'n03970156', 'n03976657', 'n03977966', 'n03980874', 'n03983396', 'n03992509', 'n04008634', 'n04023962', 'n04067472', 'n04070727', 'n04074963', 'n04099969', 'n04118538', 'n04133789', 'n04146614', 'n04149813', 'n04179913', 'n04251144', 'n04254777', 'n04259630', 'n04265275', 'n04275548', 'n04285008', 'n04311004', 'n04328186', 'n04356056', 'n04366367', 'n04371430', 'n04376876', 'n04398044', 'n04399382', 'n04417672', 'n04456115', 'n04465501', 'n04486054', 'n04487081', 'n04501370', 'n04507155', 'n04532106', 'n04532670', 'n04540053', 'n04560804', 'n04562935', 'n04596742', 'n04597913', 'n06596364', 'n07579787', 'n07583066', 'n07614500', 'n07615774', 'n07695742', 'n07711569', 'n07715103', 'n07720875', 'n07734744', 'n07747607', 'n07749582', 'n07753592', 'n07768694', 'n07871810', 'n07873807', 'n07875152', 'n07920052', 'n09193705', 'n09246464', 'n09256479', 'n09332890', 'n09428293', 'n12267677']\n",
            "Validation dataset classes: ['n01443537', 'n01629819', 'n01641577', 'n01644900', 'n01698640', 'n01742172', 'n01768244', 'n01770393', 'n01774384', 'n01774750', 'n01784675', 'n01855672', 'n01882714', 'n01910747', 'n01917289', 'n01944390', 'n01945685', 'n01950731', 'n01983481', 'n01984695', 'n02002724', 'n02056570', 'n02058221', 'n02074367', 'n02085620', 'n02094433', 'n02099601', 'n02099712', 'n02106662', 'n02113799', 'n02123045', 'n02123394', 'n02124075', 'n02125311', 'n02129165', 'n02132136', 'n02165456', 'n02190166', 'n02206856', 'n02226429', 'n02231487', 'n02233338', 'n02236044', 'n02268443', 'n02279972', 'n02281406', 'n02321529', 'n02364673', 'n02395406', 'n02403003', 'n02410509', 'n02415577', 'n02423022', 'n02437312', 'n02480495', 'n02481823', 'n02486410', 'n02504458', 'n02509815', 'n02666196', 'n02669723', 'n02699494', 'n02730930', 'n02769748', 'n02788148', 'n02791270', 'n02793495', 'n02795169', 'n02802426', 'n02808440', 'n02814533', 'n02814860', 'n02815834', 'n02823428', 'n02837789', 'n02841315', 'n02843684', 'n02883205', 'n02892201', 'n02906734', 'n02909870', 'n02917067', 'n02927161', 'n02948072', 'n02950826', 'n02963159', 'n02977058', 'n02988304', 'n02999410', 'n03014705', 'n03026506', 'n03042490', 'n03085013', 'n03089624', 'n03100240', 'n03126707', 'n03160309', 'n03179701', 'n03201208', 'n03250847', 'n03255030', 'n03355925', 'n03388043', 'n03393912', 'n03400231', 'n03404251', 'n03424325', 'n03444034', 'n03447447', 'n03544143', 'n03584254', 'n03599486', 'n03617480', 'n03637318', 'n03649909', 'n03662601', 'n03670208', 'n03706229', 'n03733131', 'n03763968', 'n03770439', 'n03796401', 'n03804744', 'n03814639', 'n03837869', 'n03838899', 'n03854065', 'n03891332', 'n03902125', 'n03930313', 'n03937543', 'n03970156', 'n03976657', 'n03977966', 'n03980874', 'n03983396', 'n03992509', 'n04008634', 'n04023962', 'n04067472', 'n04070727', 'n04074963', 'n04099969', 'n04118538', 'n04133789', 'n04146614', 'n04149813', 'n04179913', 'n04251144', 'n04254777', 'n04259630', 'n04265275', 'n04275548', 'n04285008', 'n04311004', 'n04328186', 'n04356056', 'n04366367', 'n04371430', 'n04376876', 'n04398044', 'n04399382', 'n04417672', 'n04456115', 'n04465501', 'n04486054', 'n04487081', 'n04501370', 'n04507155', 'n04532106', 'n04532670', 'n04540053', 'n04560804', 'n04562935', 'n04596742', 'n04597913', 'n06596364', 'n07579787', 'n07583066', 'n07614500', 'n07615774', 'n07695742', 'n07711569', 'n07715103', 'n07720875', 'n07734744', 'n07747607', 'n07749582', 'n07753592', 'n07768694', 'n07871810', 'n07873807', 'n07875152', 'n07920052', 'n09193705', 'n09246464', 'n09256479', 'n09332890', 'n09428293', 'n12267677']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n",
            "100%|██████████| 548M/548M [00:02<00:00, 213MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:01<00:00, 222MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/11\n",
            "----------\n",
            "train Loss: 2.5200 Acc: 0.4862\n",
            "val Loss: 0.9400 Acc: 0.7677\n",
            "Epoch 1/11\n",
            "----------\n",
            "train Loss: 1.5467 Acc: 0.6326\n",
            "val Loss: 0.7599 Acc: 0.8014\n",
            "Epoch 2/11\n",
            "----------\n",
            "train Loss: 1.4274 Acc: 0.6570\n",
            "val Loss: 0.7087 Acc: 0.8118\n",
            "Epoch 3/11\n",
            "----------\n",
            "train Loss: 1.3697 Acc: 0.6693\n",
            "val Loss: 0.6876 Acc: 0.8200\n",
            "Epoch 4/11\n",
            "----------\n",
            "train Loss: 1.3289 Acc: 0.6789\n",
            "val Loss: 0.6728 Acc: 0.8237\n",
            "Epoch 5/11\n",
            "----------\n",
            "train Loss: 1.3232 Acc: 0.6806\n",
            "val Loss: 0.6675 Acc: 0.8224\n",
            "Epoch 6/11\n",
            "----------\n",
            "train Loss: 1.3085 Acc: 0.6835\n",
            "val Loss: 0.6643 Acc: 0.8241\n",
            "Epoch 7/11\n",
            "----------\n",
            "train Loss: 1.3066 Acc: 0.6830\n",
            "val Loss: 0.6623 Acc: 0.8245\n",
            "Epoch 8/11\n",
            "----------\n",
            "train Loss: 1.3013 Acc: 0.6837\n",
            "val Loss: 0.6618 Acc: 0.8247\n",
            "Epoch 9/11\n",
            "----------\n",
            "train Loss: 1.3047 Acc: 0.6822\n",
            "val Loss: 0.6608 Acc: 0.8248\n",
            "Epoch 10/11\n",
            "----------\n",
            "train Loss: 1.3024 Acc: 0.6835\n",
            "val Loss: 0.6605 Acc: 0.8248\n",
            "Epoch 11/11\n",
            "----------\n",
            "train Loss: 1.3016 Acc: 0.6836\n",
            "val Loss: 0.6602 Acc: 0.8247\n",
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "# Define the path in Google Drive where you want to save the model\n",
        "save_dir = \"/content/drive/My Drive/saved_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# Save the model\n",
        "model_path = os.path.join(save_dir, \"hybrid_vgg_vit_tiny_imagenet.pth\")\n",
        "torch.save(model_ft.state_dict(), model_path)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1krxDJRtC-q",
        "outputId": "de6556a2-73d6-46a6-9d1d-7342362e9314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model saved to /content/drive/My Drive/saved_models/hybrid_vgg_vit_tiny_imagenet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refined Knowledge Distillation Loss\n",
        "class KnowledgeDistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=3.0, alpha=0.7):\n",
        "        super(KnowledgeDistillationLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self.ce_loss = nn.CrossEntropyLoss()  # Hard target loss\n",
        "        self.kl_loss = nn.KLDivLoss(reduction=\"batchmean\")  # Soft target loss\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, labels):\n",
        "        # Compute soft targets (teacher logits scaled by temperature)\n",
        "        soft_teacher = torch.nn.functional.softmax(teacher_logits / self.temperature, dim=1)\n",
        "        soft_student = torch.nn.functional.log_softmax(student_logits / self.temperature, dim=1)\n",
        "        kl_div = self.kl_loss(soft_student, soft_teacher)\n",
        "\n",
        "        # Hard target loss\n",
        "        ce_loss = self.ce_loss(student_logits, labels)\n",
        "\n",
        "        # Combined loss\n",
        "        return self.alpha * kl_div * (self.temperature ** 2) + (1 - self.alpha) * ce_loss\n",
        "\n",
        "# Initialize Student Model\n",
        "def initialize_student_model(num_classes):\n",
        "    student_model = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")  # Use the recommended weights\n",
        "    student_model.classifier[1] = nn.Linear(student_model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    # Optionally freeze some layers to stabilize training\n",
        "    for param in student_model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return student_model.to(device)\n",
        "\n",
        "# Training Knowledge Distillation\n",
        "def train_knowledge_distillation(teacher_model, student_model, dataloaders, criterion, optimizer, scheduler, num_epochs=12, patience=5):\n",
        "    best_model_wts = copy.deepcopy(student_model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                teacher_model.eval()  # Teacher is fixed\n",
        "                student_model.train()\n",
        "            else:\n",
        "                student_model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    teacher_logits = teacher_model(inputs)  # Teacher predictions\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    student_logits = student_model(inputs)  # Student predictions\n",
        "                    loss = criterion(student_logits, teacher_logits, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, preds = torch.max(student_logits, 1)\n",
        "                correct += torch.sum(preds == labels.data)\n",
        "                total += labels.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = correct.double() / total\n",
        "\n",
        "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(student_model.state_dict())\n",
        "                early_stop_counter = 0\n",
        "            elif phase == 'val':\n",
        "                early_stop_counter += 1\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    student_model.load_state_dict(best_model_wts)\n",
        "    print(\"Knowledge Distillation complete\")\n",
        "    return student_model\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters\n",
        "    num_classes = 200  # Update for your dataset\n",
        "    num_epochs = 12\n",
        "    patience = 5\n",
        "    lr = 0.001\n",
        "\n",
        "    # Teacher Model (Already trained HybridModel loaded)\n",
        "    teacher_model = model_ft.to(device)  # Replace `model_ft` with your loaded teacher model variable\n",
        "\n",
        "    # Student Model\n",
        "    student_model = initialize_student_model(num_classes)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    distillation_loss = KnowledgeDistillationLoss(temperature=3.0, alpha=0.7)\n",
        "    optimizer = optim.Adam(student_model.parameters(), lr=lr)  # Use Adam for better stability\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "\n",
        "    # Train the student model\n",
        "    trained_student_model = train_knowledge_distillation(\n",
        "        teacher_model, student_model, dataloaders_dict, distillation_loss, optimizer, scheduler, num_epochs=num_epochs, patience=patience\n",
        "    )\n",
        "\n",
        "    # Save the trained student model\n",
        "    save_dir = \"/content/drive/My Drive/saved_models\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    student_model_path = os.path.join(save_dir, \"efficientnet_student_fixed.pth\")\n",
        "    torch.save(trained_student_model.state_dict(), student_model_path)\n",
        "    print(f\"Student model saved to {student_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0XGpv1fyna0",
        "outputId": "3482d6a4-e403-4dd2-810c-fa7da662ca54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:00<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 3.4648 Acc: 0.2712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 3.0470 Acc: 0.4665\n",
            "Epoch 1/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:01<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.8888 Acc: 0.3339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.7646 Acc: 0.4951\n",
            "Epoch 2/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:01<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.8222 Acc: 0.3414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.6188 Acc: 0.5165\n",
            "Epoch 3/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:02<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7964 Acc: 0.3467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.6057 Acc: 0.5182\n",
            "Epoch 4/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:02<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7568 Acc: 0.3561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5622 Acc: 0.5278\n",
            "Epoch 5/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:02<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7483 Acc: 0.3558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5536 Acc: 0.5273\n",
            "Epoch 6/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:02<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7416 Acc: 0.3561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5616 Acc: 0.5234\n",
            "Epoch 7/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:02<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7293 Acc: 0.3600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5594 Acc: 0.5246\n",
            "Epoch 8/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:01<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7354 Acc: 0.3582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5520 Acc: 0.5256\n",
            "Epoch 9/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:01<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7349 Acc: 0.3591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5101 Acc: 0.5316\n",
            "Epoch 10/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:01<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7317 Acc: 0.3568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5670 Acc: 0.5241\n",
            "Epoch 11/11\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [08:01<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.7352 Acc: 0.3584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:45<00:00,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 2.5723 Acc: 0.5219\n",
            "Knowledge Distillation complete\n",
            "Student model saved to /content/drive/My Drive/saved_models/efficientnet_student_fixed.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, num_classes):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate Teacher Model\n",
        "print(\"\\nEvaluating Teacher Model:\")\n",
        "teacher_accuracy, teacher_precision, teacher_recall, teacher_f1 = evaluate_model(teacher_model, dataloaders_dict[\"val\"], num_classes)\n",
        "print(f\"Teacher Model - Accuracy: {teacher_accuracy:.4f}, Precision: {teacher_precision:.4f}, Recall: {teacher_recall:.4f}, F1-Score: {teacher_f1:.4f}\")\n",
        "\n",
        "# Evaluate Student Model\n",
        "print(\"\\nEvaluating Student Model:\")\n",
        "student_accuracy, student_precision, student_recall, student_f1 = evaluate_model(trained_student_model, dataloaders_dict[\"val\"], num_classes)\n",
        "print(f\"Student Model - Accuracy: {student_accuracy:.4f}, Precision: {student_precision:.4f}, Recall: {student_recall:.4f}, F1-Score: {student_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riNuc1wLMWYZ",
        "outputId": "f9d747eb-6e26-4fe3-c62e-ac55fdd1b8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Teacher Model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 79/79 [00:43<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model - Accuracy: 0.8247, Precision: 0.8280, Recall: 0.8247, F1-Score: 0.8247\n",
            "\n",
            "Evaluating Student Model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 79/79 [00:18<00:00,  4.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model - Accuracy: 0.5316, Precision: 0.5385, Recall: 0.5316, F1-Score: 0.5233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}